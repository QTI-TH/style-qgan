{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"classical-classical_adadelta.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPWcE9NiQIRGQADK2DoIPlK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"2CTHfsOeTXyu","executionInfo":{"status":"ok","timestamp":1618923620234,"user_tz":-120,"elapsed":3331,"user":{"displayName":"Carlos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHCP4JJjuhE_YCNKpor-5yF5Dx1752FQLpu89gjg=s64","userId":"16136423340632863528"}}},"source":["# train a classical generative adversarial network on a gaussian probability distribution\n","import numpy as np\n","from numpy.random import rand\n","from numpy.random import randn\n","from keras.models import Sequential, model_from_json\n","from keras.optimizers import Adadelta\n","from tensorflow.keras.layers import Dense, Conv2D, Conv2DTranspose, Dropout, Reshape, LeakyReLU, Flatten, BatchNormalization\n","from matplotlib import pyplot"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ofm3E_dXTdbk","executionInfo":{"status":"ok","timestamp":1618923620623,"user_tz":-120,"elapsed":3716,"user":{"displayName":"Carlos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHCP4JJjuhE_YCNKpor-5yF5Dx1752FQLpu89gjg=s64","userId":"16136423340632863528"}}},"source":["# define the standalone discriminator model\n","def define_discriminator(n_inputs=1, alpha=0.2, dropout=0.2):\n","    model = Sequential()\n","        \n","    model.add(Dense(200, use_bias=False, input_dim=n_inputs))\n","    model.add(Reshape((10,10,2)))\n","    \n","    model.add(Conv2D(64, kernel_size=3, strides=1, padding='same', kernel_initializer='glorot_normal'))\n","    model.add(LeakyReLU(alpha=alpha))\n","    \n","    model.add(Conv2D(32, kernel_size=3, strides=1, padding='same', kernel_initializer='glorot_normal'))\n","    model.add(LeakyReLU(alpha=alpha))\n","\n","    model.add(Conv2D(16, kernel_size=3, strides=1, padding='same', kernel_initializer='glorot_normal'))\n","    model.add(LeakyReLU(alpha=alpha))\n","\n","    model.add(Conv2D(8, kernel_size=3, strides=1, padding='same', kernel_initializer='glorot_normal'))\n","\n","    model.add(Flatten())\n","    model.add(LeakyReLU(alpha=alpha))\n","    model.add(Dropout(dropout)) \n","\n","    model.add(Dense(1, activation='sigmoid'))\n","    \n","    # compile model\n","    opt = Adadelta(learning_rate=0.0001)\n","    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","    return model\n"," \n","# define the standalone generator model\n","def define_generator(latent_dim, n_outputs=1, alpha=0.2):\n","    model = Sequential()\n","\n","    model.add(Dense(200, input_dim=latent_dim, kernel_initializer='glorot_normal'))\n","    model.add(BatchNormalization())\n","    model.add(LeakyReLU(alpha=alpha))\n","    model.add(Reshape((10,10,2)))\n","\n","    model.add(Conv2DTranspose(32, kernel_size=2, strides=1, padding=('same'), use_bias=False, kernel_initializer='glorot_normal'))\n","    model.add(BatchNormalization())\n","    model.add(LeakyReLU(alpha=alpha))\n","\n","    model.add(Conv2DTranspose(16, kernel_size=2, strides=1, padding=('same'), use_bias=False, kernel_initializer='glorot_normal'))\n","    model.add(BatchNormalization())\n","    model.add(LeakyReLU(alpha=alpha))\n","\n","    model.add(Conv2DTranspose(8, kernel_size=3, strides=1, padding=('same'), use_bias=False, kernel_initializer='glorot_normal'))\n","    model.add(BatchNormalization())\n","    model.add(LeakyReLU(alpha=alpha))\n","\n","    model.add(Flatten())\n","\n","    model.add(Dense(n_outputs, activation='tanh'))\n","    \n","    return model\n"," \n","# define the combined generator and discriminator model, for updating the generator\n","def define_gan(generator, discriminator):\n","    # make weights in the discriminator not trainable\n","    discriminator.trainable = False\n","    # connect them\n","    model = Sequential()\n","    # add generator\n","    model.add(generator)\n","    # add the discriminator\n","    model.add(discriminator)\n","    # compile model\n","    opt = Adadelta(learning_rate=0.0001)\n","    model.compile(loss='binary_crossentropy', optimizer=opt)\n","    return model\n"," \n","# generate real samples with class labels\n","def generate_real_samples(samples, sigma=0.25, mu=0.0):\n","    # generate samples from the distribution\n","    s = np.random.normal(mu, sigma, samples)\n","    # shape array\n","    X = s.reshape(samples, 1)\n","    # generate class labels\n","    y = np.ones((samples, 1))\n","    return X, y\n"," \n","# generate points in latent space as input for the generator\n","def generate_latent_points(latent_dim, samples):\n","    # generate points in the latent space\n","    x_input = randn(latent_dim * samples)\n","    # reshape into a batch of inputs for the network\n","    x_input = x_input.reshape(samples, latent_dim)\n","    return x_input\n"," \n","# use the generator to generate fake examples, with class labels\n","def generate_fake_samples(generator, latent_dim, samples):\n","    # generate points in latent space\n","    x_input = generate_latent_points(latent_dim, samples)\n","    # predict outputs\n","    X = generator.predict(x_input)\n","    # shape array\n","    X = X.reshape(samples, 1)\n","    # create class labels\n","    y = np.zeros((samples, 1))\n","    return X, y\n"," \n","# evaluate the discriminator and plot real and fake samples\n","def summarize_performance(epoch, generator, discriminator, latent_dim, samples, nbins, d_loss, g_loss):\n","    # prepare real samples\n","    x_real, y_real = generate_real_samples(100000)\n","    # evaluate discriminator on real examples\n","    _, acc_real = discriminator.evaluate(x_real, y_real, verbose=0)\n","    # prepare fake examples\n","    x_fake, y_fake = generate_fake_samples(generator, latent_dim, 100000)\n","    # evaluate discriminator on fake examples\n","    _, acc_fake = discriminator.evaluate(x_fake, y_fake, verbose=0)\n","    # summarize discriminator performance\n","    print('epoch: ',epoch, 'acc_real: ', acc_real, 'acc_fake: ', acc_fake)\n","    # histogram plot real and fake data points\n","    pyplot.hist(x_real, np.linspace(-1.0, 1.0, nbins+1), color='red', label='real', alpha=0.5)\n","    pyplot.hist(x_fake, np.linspace(-1.0, 1.0, nbins+1), color='blue', label='fake', alpha=0.5)\n","    #yplot.hist(x_real, color='red', label='real', alpha=0.5)\n","    #yplot.hist(x_fake, color='blue', label='fake', alpha=0.5)\n","    pyplot.legend()\n","    pyplot.show()\n","\n","    # plot loss generator and discriminator\n","    pyplot.plot(np.linspace(0, len(g_loss), len(g_loss)), g_loss, label='generator')\n","    pyplot.plot(np.linspace(0, len(g_loss), len(g_loss)), d_loss, label='discriminator')\n","    pyplot.legend()\n","    pyplot.show()  \n","\n","    # serialize weights to HDF5\n","    generator.save_weights(str(epoch)+\"_generator.h5\")\n","    print(\"Saved generator to disk\")\n","    discriminator.save_weights(str(epoch)+\"_discriminator.h5\")\n","    print(\"Saved discriminator to disk\")\n"," \n","# train the generator and discriminator\n","def train(g_model, d_model, gan_model, latent_dim, n_epochs=30000, samples=256, nbins=49, n_eval=250):\n","    d_loss = []\n","    g_loss = []\n","    # determine half the size of one batch, for updating the discriminator\n","    half_samples = int(samples / 2)\n","    # manually enumerate epochs\n","    for i in range(n_epochs):\n","        # prepare real samples\n","        x_real, y_real = generate_real_samples(half_samples)\n","        # prepare fake examples\n","        x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_samples)\n","        # update discriminator\n","        d_model.trainable = True\n","        d_loss_real, _ = d_model.train_on_batch(x_real, y_real)\n","        d_loss_fake, _ = d_model.train_on_batch(x_fake, y_fake)\n","        d_loss.append((d_loss_real + d_loss_fake)/2)        \n","        # prepare points in latent space as input for the generator\n","        x_gan = generate_latent_points(latent_dim, samples)\n","        # create inverted labels for the fake samples\n","        y_gan = np.ones((samples, 1))\n","        # update the generator via the discriminator's error\n","        g_loss_fake = gan_model.train_on_batch(x_gan, y_gan)\n","        g_loss.append(g_loss_fake)\n","        # evaluate the model every n_eval epochs\n","        if (i+1) % n_eval == 0:\n","            summarize_performance(i, g_model, d_model, latent_dim, samples, nbins, d_loss, g_loss)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1LCQis1syHjEvmAsk0-fv-hPSJdEyqJKt"},"id":"UTTQNgHbUXyh","executionInfo":{"status":"ok","timestamp":1618945035158,"user_tz":-120,"elapsed":21416822,"user":{"displayName":"Carlos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHCP4JJjuhE_YCNKpor-5yF5Dx1752FQLpu89gjg=s64","userId":"16136423340632863528"}},"outputId":"3519d3f9-f615-4ed4-a606-e92a9ec4100e"},"source":["# size of the latent space\n","latent_dim = 5\n","# create the discriminator\n","discriminator = define_discriminator()\n","# create the generator\n","generator = define_generator(latent_dim)\n","# create the gan\n","gan_model = define_gan(generator, discriminator)\n","# serialize generator and discriminator to JSON\n","gen_model_json = generator.to_json()\n","with open(\"generator.json\", \"w\") as json_file:\n","    json_file.write(gen_model_json)\n","disc_model_json = discriminator.to_json()\n","with open(\"discriminator.json\", \"w\") as json_file:\n","    json_file.write(disc_model_json)\n","# train model\n","train(generator, discriminator, gan_model, latent_dim)"],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"Gxe4V1jtC5XF"},"source":[""],"execution_count":null,"outputs":[]}]}