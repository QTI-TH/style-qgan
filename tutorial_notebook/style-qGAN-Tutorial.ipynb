{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style-based quantum generative adversarial networks for Monte Carlo events\n",
    "\n",
    "Code at:\n",
    "\n",
    "Paper:\n",
    "\n",
    "In this tutorial, we show how to implement a style-based quantum generative adversarial network (style-qGAN). We provide the tools for training the style-qGAN on a reference example, namely, a 3D correlated Gaussian distribution. For this particular implementation, we are going to use the Qibo API for the simulation of quantum circuits, and the Keras packages for the execution of classical neural networks.\n",
    "\n",
    "## Framework\n",
    "\n",
    "The generative adversarial framework is a powerful tool for training generator models. It involves at least three required components: the discriminator model, the generator model, and the adversarial training procedure. In this tutorial, we consider a hybrid quantum-classical system, where the generator model has a quantum representation through a quantum neural network, while the discriminator is a classical neural network model.\n",
    "\n",
    "As we show in the figure below, the procedure starts from the preparation of samples from a known distribution function that we would like to encode in the quantum generator model. At the same time, we define a quantum neural network model where we inject stochastic noise in the latent space variables which are used to define quantum gates. The generator model is then used to extract fake samples that after the training procedure should match the quality of the known input distribution. Both sets of samples are then used to train the discriminator model. The quality of the training is measured by an appropriate loss function which is monitored and optimized classically by a minimization algorithm based on the adversarial approach. The training process consists of a simultaneous stochastic gradient descent for both models which after reaching convergence delivers a quantum generator model with realistic sampling.\n",
    "\n",
    "<img src=\"scheme.png\" width=\"320px\">\n",
    "\n",
    "This is a complex type of model both to understand and to train. A possible approach to better understand the nature of this style-qGAN model and how it can be trained is to develop the model from scratch.\n",
    "\n",
    "In this tutorial, we will select a simple distribution and use it as the basis for developing and evaluating a quantum generative adversarial network from scratch: a 3D correlated Gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refererence model\n",
    "\n",
    "The starting point should be generating real samples for our discriminator. Specifically, we will generate real samples from a 3D correlated Gaussian distribution. A sample is going to be comprised of a vector with three elements, one for each dimension. We will normalize this samples by dividing by 4, so each sample is within the [-1,1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "\n",
    "def generate_training_real_samples(samples):\n",
    "  # generate training samples from the distribution\n",
    "    s = []\n",
    "    mean = [0, 0, 0]\n",
    "    cov = [[0.5, 0.1, 0.25], [0.1, 0.5, 0.1], [0.25, 0.1, 0.5]]\n",
    "    x, y, z = np.random.multivariate_normal(mean, cov, samples).T/4\n",
    "    s1 = np.reshape(x, (samples,1))\n",
    "    s2 = np.reshape(y, (samples,1))\n",
    "    s3 = np.reshape(z, (samples,1))\n",
    "    s = np.hstack((s1,s2,s3))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a discriminator model\n",
    "\n",
    "The discriminator model should take a sample from our problem, in this case a vector with three elements, and output a prediction as to whether the samples are fake or real. In this case, we will choose a deep convolutional neural network, with several hidden layers and we will use the ReLU activation function. The output layer will have one node for the binary classification using the sigmoid activation function. The model will minimize the binary cross-entropy loss function, and the we will use the stochastic gradient descent method Adadelta to update the parameters.\n",
    "\n",
    "The model must take a sample from our problem, such as a vector with two elements, and output a classification prediction as to whether the sample is real or fake. This is done by the function `define_discriminator()` below.\n",
    "\n",
    "We will also implement a function that will be useful later on for the batch optimization. This function is `generate_real_samples()`, which from a particular set of samples, it chooses a batch of some particular size and gives the class label (real in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Reshape, LeakyReLU, Flatten\n",
    "\n",
    "# define the standalone discriminator model\n",
    "def define_discriminator(n_inputs=3, alpha=0.2, dropout=0.2):\n",
    "    model = Sequential()\n",
    "        \n",
    "    model.add(Dense(200, use_bias=False, input_dim=n_inputs))\n",
    "    model.add(Reshape((10,10,2)))\n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size=3, strides=1, padding='same', kernel_initializer='glorot_normal'))\n",
    "    model.add(LeakyReLU(alpha=alpha))\n",
    "    \n",
    "    model.add(Conv2D(32, kernel_size=3, strides=1, padding='same', kernel_initializer='glorot_normal'))\n",
    "    model.add(LeakyReLU(alpha=alpha))\n",
    "\n",
    "    model.add(Conv2D(16, kernel_size=3, strides=1, padding='same', kernel_initializer='glorot_normal'))\n",
    "    model.add(LeakyReLU(alpha=alpha))\n",
    "\n",
    "    model.add(Conv2D(8, kernel_size=3, strides=1, padding='same', kernel_initializer='glorot_normal'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(LeakyReLU(alpha=alpha))\n",
    "    model.add(Dropout(dropout)) \n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # compile model\n",
    "    opt = Adadelta(learning_rate=0.1)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create classical discriminator\n",
    "discriminator = define_discriminator()\n",
    "\n",
    "# generate real samples with class labels\n",
    "def generate_real_samples(samples, distribution, real_samples):\n",
    "    # generate samples from the distribution\n",
    "    idx = np.random.randint(real_samples, size=samples)\n",
    "    X = distribution[idx,:]\n",
    "    # generate class labels\n",
    "    y = np.ones((samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a quantum generator model\n",
    "\n",
    "Before defining the generator model, we first should provide with some of the tools needed to create it. Specifically, we should provide the number of qubits (which is 3), and what expectation values do we want to generate the fake samples (`hamiltonian1()`, `hamiltonian2()`, `hamiltonian3()`) that will act on each of the qubits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "from qibo import gates, hamiltonians, models, set_backend\n",
    "\n",
    "# set qibo backend\n",
    "set_backend('tensorflow')\n",
    "\n",
    "# number of qubits generator\n",
    "nqubits = 3\n",
    "\n",
    "# define hamiltonian to generate fake samples\n",
    "def hamiltonian1():\n",
    "    id = [[1, 0], [0, 1]]\n",
    "    m0 = hamiltonians.Z(1, numpy=True).matrix\n",
    "    m0 = np.kron(id, np.kron(id, m0))\n",
    "    ham = hamiltonians.Hamiltonian(3, m0)\n",
    "    return ham\n",
    "\n",
    "def hamiltonian2():\n",
    "    id = [[1, 0], [0, 1]]\n",
    "    m0 = hamiltonians.Z(1, numpy=True).matrix\n",
    "    m0 = np.kron(id, np.kron(m0, id))\n",
    "    ham = hamiltonians.Hamiltonian(3, m0)\n",
    "    return ham\n",
    "\n",
    "def hamiltonian3():\n",
    "    id = [[1, 0], [0, 1]]\n",
    "    m0 = hamiltonians.Z(1, numpy=True).matrix\n",
    "    m0 = np.kron(m0, np.kron(id, id))\n",
    "    ham = hamiltonians.Hamiltonian(3, m0)\n",
    "    return ham\n",
    "\n",
    "# create hamiltonians\n",
    "hamiltonian1 = hamiltonian1()\n",
    "hamiltonian2 = hamiltonian2()\n",
    "hamiltonian3 = hamiltonian3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantum generator model will take as input a set of latent variables and generate a new sample, in this case, a vector with three components. Latent variables are variables that are not directly observed but will be rather inferred by the quantum generator model. This is because the latent space has no meaning until the quantum generator model starts assigning meaning to points in the space as it learns. We will define a latent space of three dimensions and use the standard approach from the classical GAN literature by using a standard Gaussian distribution for each variable in the latent space. This is done by the function `generate_latent_points()`. Then, we will generate new fake samples by implementing our quantum circuit (see Figure below) with some particular number of layers, and using the function `generate_fake_samples()`.\n",
    "\n",
    "<img src=\"ansatz.png\" width=\"550px\">\n",
    "\n",
    "The latent variables $\\xi$ and trainable parameters $\\phi_g$ are going be to encoded inside quantum rotation gate as\n",
    "\n",
    "<img src=\"encoding.png\" width=\"350px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of layers\n",
    "layers = 1\n",
    "\n",
    "# create quantum generator\n",
    "circuit = models.Circuit(nqubits)\n",
    "for l in range(layers):\n",
    "    for q in range(nqubits):\n",
    "        circuit.add(gates.RY(q, 0))\n",
    "        circuit.add(gates.RZ(q, 0))\n",
    "        circuit.add(gates.RY(q, 0))\n",
    "        circuit.add(gates.RZ(q, 0))\n",
    "    if l==0 or l==2 or l==4 or l==6 or l==8:\n",
    "        circuit.add(gates.CRY(0, 1, 0))\n",
    "        circuit.add(gates.CRY(0, 2, 0))\n",
    "    if l==1 or l==3 or l==5 or l==7 or l==9:\n",
    "        circuit.add(gates.CRY(1, 2, 0))\n",
    "        circuit.add(gates.CRY(2, 0, 0))\n",
    "for q in range(nqubits):\n",
    "    circuit.add(gates.RY(q, 0))\n",
    "    \n",
    "def set_params(circuit, params, x_input, i, nqubits, layers, latent_dim):\n",
    "    p = []\n",
    "    index = 0\n",
    "    noise = 0\n",
    "    for l in range(layers):\n",
    "        for q in range(nqubits):\n",
    "            p.append(params[index]*x_input[noise][i] + params[index+1])\n",
    "            index+=2\n",
    "            noise=(noise+1)%latent_dim\n",
    "            p.append(params[index]*x_input[noise][i] + params[index+1])\n",
    "            index+=2\n",
    "            p.append(params[index]*x_input[noise][i] + params[index+1])\n",
    "            index+=2\n",
    "            noise=(noise+1)%latent_dim\n",
    "            p.append(params[index]*x_input[noise][i] + params[index+1])\n",
    "            index+=2\n",
    "            noise=(noise+1)%latent_dim\n",
    "        if l==0 or l==2 or l==4 or l==6 or l==8:\n",
    "            p.append(params[index]*x_input[noise][i] + params[index+1])\n",
    "            index+=2\n",
    "            noise=(noise+1)%latent_dim\n",
    "            p.append(params[index]*x_input[noise][i] + params[index+1])\n",
    "            index+=2\n",
    "            noise=(noise+1)%latent_dim\n",
    "        if l==1 or l==3 or l==5 or l==7 or l==9:\n",
    "            p.append(params[index]*x_input[noise][i] + params[index+1])\n",
    "            index+=2\n",
    "            noise=(noise+1)%latent_dim\n",
    "            p.append(params[index]*x_input[noise][i] + params[index+1])\n",
    "            index+=2\n",
    "            noise=(noise+1)%latent_dim\n",
    "    for q in range(nqubits):\n",
    "        p.append(params[index]*x_input[noise][i] + params[index+1])\n",
    "        index+=2\n",
    "        noise=(noise+1)%latent_dim\n",
    "    circuit.set_parameters(p) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, samples):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(samples, latent_dim)\n",
    "    return x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the generator to generate fake examples, with class labels\n",
    "def generate_fake_samples(params, latent_dim, samples, circuit, nqubits, layers, hamiltonian1, hamiltonian2, hamiltonian3):\n",
    "    # generate points in latent space\n",
    "    x_input = generate_latent_points(latent_dim, samples)\n",
    "    x_input = np.transpose(x_input)\n",
    "    # generator outputs\n",
    "    X1 = []\n",
    "    X2 = []\n",
    "    X3 = []\n",
    "    # quantum generator circuit\n",
    "    for i in range(samples):\n",
    "        set_params(circuit, params, x_input, i, nqubits, layers, latent_dim)\n",
    "        circuit_execute = circuit.execute()\n",
    "        X1.append(hamiltonian1.expectation(circuit_execute))\n",
    "        X2.append(hamiltonian2.expectation(circuit_execute))\n",
    "        X3.append(hamiltonian3.expectation(circuit_execute))\n",
    "    # shape array\n",
    "    X = tf.stack((X1, X2, X3), axis=1)\n",
    "    # create class labels\n",
    "    y = np.zeros((samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the adversarial model\n",
    "\n",
    "Here, the trainable parameters of the quantum generator model and classical discriminator model are going to be updated based on the performance of each other. This defines a zero-sum adversarial game between the models. The simplest approach is to create a new function that encapsulates the generation of fake samples and the evaluation of the discriminator model. This function is `define_cost_gan()`. The generator will receive as input random points in the latent space, generate fake samples and then fed them into the discriminator model. The output will be used to update the trainable parameters of the generator.\n",
    "\n",
    "The discriminator is concerned with distinguishing the real and fake samples, and therefore, can be trained in a standalone manner. Both the training of the discriminator and quantum generator is done in the function `train()`. Once both models are trained, we obtain a quantum generator capable of reproducing the reference distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_cost_gan(params, discriminator, latent_dim, samples, circuit, nqubits, layers, hamiltonian1, hamiltonian2, hamiltonian3):\n",
    "    # generate fake samples\n",
    "    x_fake, y_fake = generate_fake_samples(params, latent_dim, samples, circuit, nqubits, layers, hamiltonian1, hamiltonian2, hamiltonian3)\n",
    "    # create inverted labels for the fake samples\n",
    "    y_fake = np.ones((samples, 1))\n",
    "    # evaluate discriminator on fake examples\n",
    "    disc_output = discriminator(x_fake)\n",
    "    loss = tf.keras.losses.binary_crossentropy(y_fake, disc_output)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the generator and discriminator\n",
    "def train(d_model, latent_dim, layers, nqubits, training_samples, discriminator, circuit, n_epochs, samples, lr, hamiltonian1, hamiltonian2, hamiltonian3):\n",
    "    d_loss = []\n",
    "    g_loss = []\n",
    "    # determine half the size of one batch, for updating the discriminator\n",
    "    half_samples = int(samples / 2)\n",
    "    initial_params = tf.Variable(np.random.uniform(-0.15, 0.15, 8*layers*nqubits + 4*layers + 2*nqubits))\n",
    "    optimizer = tf.optimizers.Adadelta(learning_rate=lr)\n",
    "    # prepare real samples\n",
    "    s = generate_training_real_samples(training_samples)\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "        # prepare real samples\n",
    "        x_real, y_real = generate_real_samples(half_samples, s, training_samples)\n",
    "        # prepare fake examples\n",
    "        x_fake, y_fake = generate_fake_samples(initial_params, latent_dim, half_samples, circuit, nqubits, layers, hamiltonian1, hamiltonian2, hamiltonian3)\n",
    "        # update discriminator\n",
    "        d_loss_real, _ = d_model.train_on_batch(x_real, y_real)\n",
    "        d_loss_fake, _ = d_model.train_on_batch(x_fake, y_fake)\n",
    "        d_loss.append((d_loss_real + d_loss_fake)/2)\n",
    "        # update generator\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = define_cost_gan(initial_params, d_model, latent_dim, samples, circuit, nqubits, layers, hamiltonian1, hamiltonian2, hamiltonian3)\n",
    "        grads = tape.gradient(loss, initial_params)\n",
    "        optimizer.apply_gradients([(grads, initial_params)])\n",
    "        g_loss.append(loss)\n",
    "        np.savetxt(f\"PARAMS_3Dgaussian_{nqubits}_{latent_dim}_{layers}_{training_samples}_{samples}_{lr}\", [initial_params.numpy()], newline='')\n",
    "        np.savetxt(f\"dloss_3Dgaussian_{nqubits}_{latent_dim}_{layers}_{training_samples}_{samples}_{lr}\", [d_loss], newline='')\n",
    "        np.savetxt(f\"gloss_3Dgaussian_{nqubits}_{latent_dim}_{layers}_{training_samples}_{samples}_{lr}\", [g_loss], newline='')\n",
    "        # serialize weights to HDF5\n",
    "        discriminator.save_weights(f\"discriminator_3Dgaussian_{nqubits}_{latent_dim}_{layers}_{training_samples}_{samples}_{lr}.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can select the training setup and start the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training setup\n",
    "latent_dim = 3\n",
    "training_samples = 10000\n",
    "batch_samples = 128\n",
    "n_epochs = 30000\n",
    "lr = 0.5\n",
    "\n",
    "# train model\n",
    "train(discriminator, latent_dim, layers, nqubits, training_samples, discriminator, circuit, n_epochs, batch_samples, lr, hamiltonian1, hamiltonian2, hamiltonian3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
